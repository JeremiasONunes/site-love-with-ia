import { InferenceClient } from "@huggingface/inference";

// Cria o cliente usando o token da variÃ¡vel de ambiente
const client = new InferenceClient(import.meta.env.VITE_HF_TOKEN);

export const gerarMensagemPositiva = async () => {
  try {
    const chatCompletion = await client.chatCompletion({
      provider: "auto", // provider certo para o modelo
      model: "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      messages: [
        {
          role: "user",
          content: "Escreva uma mensagem curta, doce, Ãºnica e positiva para animar minha namorada. Seja fofa, gentil e criativa. Gere a mensagem como se fosse um namorado apaixonado. Use emojis para deixar mais carinhoso. A mensagem deve ser original e nÃ£o copiar de outros lugares.coloque a mensagem entre aspas duplas.",
        },
      ],
      // VocÃª pode adicionar temperature, max_tokens etc aqui se quiser
    });

    // Retorna o conteÃºdo da primeira resposta
    return chatCompletion.choices[0].message.content || "NÃ£o consegui gerar a mensagem :(";
  } catch (erro) {
    console.error("Erro ao gerar mensagem com Hugging Face:", erro);
    return "Erro ao gerar mensagem ðŸ˜¢";
  }
};
